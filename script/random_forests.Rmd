<<<<<<< HEAD
---
title: "Week 6"
author: "Xuan Pham"
date: "`r Sys.Date()`"
output: html_document
---

# R Packages 

The packages you will need to install for this week include **caret**, **RandomForest**, **mlbench**, **adabag**, **ROCR**, and **doSNOW**.

# Model Specification & Selection: Questions to Ask

  1. What's my intended goal/outcome?    
  2. What data do I have?    
  3. Which predictors are significant?  
**4. What's the expected performance of a model?**  
**5. How do I improve a model's performance?**

We will focus on Questions 4 & 5 tonight. 

# What's the Expected Performance of a Model? 

The best way to measure performance is to know the **true error rate**. The true error rate is calculated by comparing the model's predictions against actual outcomes in the **entire population**.  In reality, we usually are not working with the whole population. We are working with one or more samples from the population. 

## Naive Approach 

A *naive* way to estimate the true error rate is to apply our model to the entire sample (i.e. training dataset) and then calculate the error rate. The naive approach has several drawbacks:

* Final model will overfit the training data. The problem is magnified when a model has a large number of parameters.    
* Estimated error rate is likely to be lower than the true error rate.

A better approach than the naive method is resampling.   

# Resampling   

Resampling refers to drawing repeated samples from the sample(s) we have. The goal of resampling is to gauge performances of competing models. Resampling is our attempt to simulate the conditions needed to calculate the true error rate.  

We will discuss three resampling methods this week:  

1. one-fold cross validation  
2. k-fold cross validation    
3. leave-one-out cross validation    
4. bootstrapping  


## Validation Set

We discussed validation set approach in Week 2 when we covered decision trees. In particular, the validation set approach involves randomly dividing the available observations into two subgroups: a) a **training set** and b) a **validation set**. We fit our model with the training set and then tests the model's performance on the validation set. Common splits include 60-40 (60% training set and 40% test set), 70-30, and 80-20.

### Test Set

The validation set is there to help us simulate the process of estimating the true error true. The validation set effectively becomes part of the training set when we use it to gauge our model's performance. If we want to measure the expected performance of a model, we should also deploy a third subset called a "test set." 


# Opioid Prescriber: A Redux!

Remember that our target variable is Opioid.Prescriber, where "1" means a practitioner is a frequent opioid prescriber and "0" means s/he is not a frequent opioid prescriber.  

```{r}
setwd("C:/Users/PhamX/Courses/Spring_2017/BIA_6301/Module_6/data")
prescribers<-read.csv("prescribers.csv")

#View(prescribers)

prescribers<-prescribers[,c(241,1:240,242:331)] #Rearranging the columns so that our target variable is first

dim(prescribers)
#names(prescribers)

table(prescribers$Opioid.Prescriber)
```

Let's do a training set of 80% and validation set of 20%. We will build a decision tree model on 80% of the data set and then test the model's performance on the other 20% of the data set.   

```{r}
set.seed(123) #set a seed to do draws from a random uniform distribution.
prescribers_rand <- prescribers[order(runif(25000)), ] 
prescribers_train <- prescribers_rand[1:20000, ] #Training data set; 20000 observations
prescribers_validate  <-prescribers_rand[20001:25000, ]
```

Checking the proportions of opioid prescriber in the trainining and validation sets. They are roughly the same. 58.8% in training set are opioid prescribers; 58.3% in validation set are opioid prescribers.

```{r}
dim(prescribers_train) #checking the split
dim(prescribers_validate) #checking the split
prop.table(table(prescribers_train$Opioid.Prescriber)) #checking to see the class proportions between the training and test sets. 
prop.table(table(prescribers_validate$Opioid.Prescriber))
```


```{r}
library(rpart)
library(rpart.plot)

set.seed(123)
prescribers_rpart <- rpart(prescribers_train$Opioid.Prescriber~., method="class", parms = list(split="gini"), data=prescribers_train)

plot(prescribers_rpart, uniform=TRUE, main="Classification Tree for Opioid Prescribers")
text(prescribers_rpart, use.n=TRUE, all=TRUE, cex=0.8)


# Something a bit fancier
library(rpart.plot)
rpart.plot(prescribers_rpart, type=0, extra=101)
rpart.plot(prescribers_rpart, type=1, extra=101)
```

```{r}
library(caret)
actual <- prescribers_validate$Opioid.Prescriber
predicted <- predict(prescribers_rpart, prescribers_validate, type="class")
results.matrix <- confusionMatrix(predicted, actual, positive="yes")
print(results.matrix)
```
Accuracy rate is 76%, but the sensitivity (how well does the decision tree classified frequent opioid prescribers correctly) is 70%. Furthermore, the specificity (how well does the decision tree classified non-frequent opioid prescribers correctly) is 84%. Our decision tree model does a better job classifying the non-frequent opioid prescribers than the frequent opioid prescribers. 


### Using the Caret Package Train & Validate Models

```{r}
set.seed(123)
trainIndex <- createDataPartition(prescribers$Opioid.Prescriber, p = .8,list = FALSE,times = 1)
prescribers_train_caret <- prescribers[ trainIndex,]
prescribers_validate_caret <- prescribers[ -trainIndex,]

```

```{r}

prescribers_rpart_caret <- rpart(Opioid.Prescriber~., method="class", parms = list(split="gini"), data=prescribers_train_caret)

plot(prescribers_rpart_caret, uniform=TRUE, main="Classification Tree for Opioid Prescribers")
text(prescribers_rpart_caret, use.n=TRUE, all=TRUE, cex=0.8)

rpart.plot(prescribers_rpart_caret, type=0, extra=101)
rpart.plot(prescribers_rpart_caret, type=1, extra=101)

actual <- prescribers_validate_caret$Opioid.Prescriber
predicted <- predict(prescribers_rpart_caret, prescribers_validate_caret, type="class")
results.matrix <- confusionMatrix(predicted, actual, positive="yes")
print(results.matrix)
```

Accuracy = 78%.  
Sensitivity = 79%  
Specificity = 74%  

The caret approach creates a test set that shows the decision tree is doing slightly better at identifying opioid prescribers. 

## Revisiting the Bias-Variance Discussion

Alpayin (2010) provided a great visualization of the bias-variance problem:  

[!]http://images.slideplayer.com/15/4846480/slides/slide_7.jpg 

When we are estimating the true error rate, we need to remember that we are dealing with the bias & variance problem too. In one fold cross validation, we have an estimated error rate that is both biased and has high variance. One way to reduce both the bias and variance is by using k-fold cross validation.


## k-Fold Cross Validation

k-fold cross validation is a resampling technique that divides the dataset into k groups, or folds, of equal size. Here is how it works:  

1. Keep one fold as the validation set. Fit the model on the other k-1 folds.  

2. Test fitted model on the validation set. Calculate the mean squared error (MSE) on the validation set. 

3. Repeat Steps 1 & 2 over and over again so that a different fold is used as a validation set. **The true error rate is estimated as the average error rate of all repetitions.**  

Use the **caret** package for this task.  

We will divide the training set into 10-folds. Each fold will eventually be used as a validation set.

```{r}

fitControl <- trainControl(method="cv", number=10) #10-fold cross validation

set.seed(123)
prescribers_10folds<-train(Opioid.Prescriber~., data=prescribers_train_caret, method="rpart", metric="Accuracy", trControl=fitControl)
prescribers_10folds
```

Using the cp value to prune our tree (if needed)
```{r}
#prescribers_rpart_pruned_10folds<-prune(prescribers_10folds, cp=0.06)
#rpart.plot(prescribers_rpart_pruned_10folds)
```

Now we calculate the error rate of the chosen decision tree on the validation set. 

```{r}
actual <- prescribers_validate_caret$Opioid.Prescriber
predicted <- predict(prescribers_10folds, prescribers_validate_caret, type="raw")
results.matrix <- confusionMatrix(predicted, actual, positive="yes")
print(results.matrix)
```

Sensitivity = 61%; Specificity = 89%


### Kappa? 

$$ Kappa = \frac{Pr(a) - Pr(e)}{1-Pr(e)} $$

Where, 

Pr(a): proportion of actual agreement between the classifier and the true values  

Pr(e): proportion of expected agreement between the classifier and the true values

Kappa "adjusts accuracy by accounting for the possibility of a correct prediction by **chance alone.** Kappa values range to a maximum number of 1, which indicates perfect agreement between the model's predictions and the true values--a rare occurrence. Values less than one indicate imperfect agreement" (Lantz 2013, p. 303)


|                   |Actual  |        |Marginal_Frequency    |
|------------------:|-------:|-------:|---------------------:|
|Prediction         |NO      |  YES   |                      |
|NO                 |1829    |1157    |2986                  |
|YES                |233     |1780    |2013                  |
|-------------------|--------|--------|----------------------|
|Marginal_Frequency |2062    |2937    |                      |


```{r}
Observed_Accuracy = (1829+1780)/5000 
Expected_Accuracy_NO = (2986*2062)/5000
Expected_Accuracy_YES = (2031*2937)/5000
Expected_Accuracy_BOTH_CLASSES = (Expected_Accuracy_NO+Expected_Accuracy_YES)/5000
Kappa_Statistic = (Observed_Accuracy-Expected_Accuracy_BOTH_CLASSES)/(1-Expected_Accuracy_BOTH_CLASSES)

table<-cbind(Observed_Accuracy,Expected_Accuracy_NO,Expected_Accuracy_YES,Expected_Accuracy_BOTH_CLASSES, Kappa_Statistic)

table_t<-t(table)

colnames(table_t)<-c("value")


library(knitr)
kable(table_t)
```

#### Side Note on Kappa Statistic

What's a good kappa value?

Landis and Koch (1977): 0-20 => slight, 0.21-.40 => fair, 0.41-0.6 => moderate, 0.61-0.8 => substantial, 0.81 -1 => almost perfect  

Fleiss (1981): 0-.40 => poor; 0.41-0.75 => fair to good; 0.75 – 1 => excellent.

Be careful! Kappa is not the best metric if accuracy is not what you are after. 


### Repeated k-fold Cross Validation

k-fold cross validation is still problematic, however. Vanwinckelen and Blockeel (2011) noted:  

`In addition to bias, the results of a k-fold cross-validation also have high variance. If we run two different tenfold cross-validations for the same learners on the same daa set S, but with different random partitioning of S into subsets S(i), these two cross-validations can give quite different results. An estimate with smaller variance can be obtained by repeating the cross-validation several times, with different partitionings, and taking the average of the results obtained during each cross-validation (page 2).`


```{r}
fitControl <- trainControl(method="cv", number=10, repeats=5) #10-fold cross validation #repeated 5 times.

set.seed(123)
prescribers_10folds_rp<-train(Opioid.Prescriber~., data=prescribers_train_caret, method="rpart", metric="Accuracy", trControl=fitControl)
prescribers_10folds_rp

actual <- prescribers_validate_caret$Opioid.Prescriber
predicted <- predict(prescribers_10folds_rp, prescribers_validate_caret, type="raw")
results.matrix <- confusionMatrix(predicted, actual, positive="yes")
print(results.matrix)
```


## Leave-one-out Cross Validation (LOOCV)

Repeated k-fold cross validation can help reduce the high variance problem, but we still have to deal with the high bias problem. A way to minimize the bias problem is to do LOOCV. The technique is a degenerate case of k-fold cross validation, where K is chosen as the total number of observations. LOOCV uses all observations as the training set and leaves one observation out as the test set. The process repeats until all observations have been used as a validation set.

LOOCV is very computationally intensive!!

```{r}

#fitControl <- trainControl(method="LOOCV") #10-fold cross validation

#set.seed(123)
#prescribers_loocv<-train(Opioid.Prescriber~., data=prescribers_train_caret, method="rpart", metric="Accuracy", trControl=fitControl)
#prescribers_loocv
```

```{r}
#actual <- prescribers_validate_caret$Opioid.Prescriber
#predicted <- predict(prescribers_loocv, prescribers_validate_caret, type="raw")
#results.matrix <- confusionMatrix(predicted, actual, positive="yes")
#print(results.matrix)
```

## Bootstrapping 

Bootstrapping is a resampling technique that obtain distinct datasets by repeatedly sampling observations from the original dataset with replacement. 

Each boostrapped dataset is created by sampling with replacement and is the same size as the original dataset. Consequently, some observations may appear more than once in a given boostrapped dataset while other observations may not appear at all.

Note: The default method in the train() function in the caret package is the bootstrap.

```{r}
cvCtrl <- trainControl(method="boot", number=10) #10 bootstrapped samples.
set.seed(123)
prescribers_bootstrap<-train(Opioid.Prescriber~., data=prescribers_train_caret, method="rpart", metric="Accuracy", trControl=cvCtrl)
prescribers_bootstrap
```

```{r}
actual <- prescribers_validate_caret$Opioid.Prescriber
predicted <- predict(prescribers_bootstrap, prescribers_validate_caret, type="raw")
results.matrix <- confusionMatrix(predicted, actual, positive="yes")
print(results.matrix)
```


## Last Words on Resampling

A question you may be pondering about is "how many folds should I use?" The answer depends on the size of the dataset. For large datasets, you can use a small number of folds and still get an accurate test error estimate. For smaller datasets, you may have to use LOOCV. You should remember these rules:


**Small number of folds** = error estimate is more biased but also has lower variance. Computationally less intensive.   

**Large number of folds** = error estimate is less biased but also has higher variance. More computationally intensive. 


# How Do I Improve Model Performance? 

For the second part of this class, we will use the **Breast Cancer** data set from the mlbench package. It's a smaller data set so easier for us to handle in class.

```{r}
library(mlbench)
data("BreastCancer")
str(BreastCancer)
BC<-BreastCancer[-1] #removing ID column
```

```{r}
set.seed(123)
trainIndex <- createDataPartition(BC$Class, p = .8,list = FALSE,times = 1)
head(trainIndex)
BC_train_caret <- BC[trainIndex,]
BC_validate_caret <- BC[trainIndex,]
```


```{r}
BC_rpart_caret <- rpart(BC_train_caret$Class~., method="class", parms = list(split="gini"), data=BC_train_caret)

plot(BC_rpart_caret, uniform=TRUE, main="Classification Tree for Opioid Prescribers")
text(BC_rpart_caret, use.n=TRUE, all=TRUE, cex=0.8)

rpart.plot(BC_rpart_caret, type=0, extra=101)
rpart.plot(BC_rpart_caret, type=1, extra=101)

actual <- BC_validate_caret$Class
predicted <- predict(BC_rpart_caret, BC_validate_caret, type="class")
results.matrix <- confusionMatrix(predicted, actual, positive="malignant")
print(results.matrix)
```

Our decision tree model does a slightly better job at predicting malignant tumors (true positives) (97%) than benign tumors (96%). This is what we want to see!


# Improving Model Performance: Ensemble Models Approach

One decision tree suffers from high variance. The resulting tree depends on the training data. What we want is a procedure with low variance--meaning we should see similar results if the tree is applied repeatedly to distinct datasets. We will examine three ensemble models that are built on the basic decision trees:

1. Bagging (bootstrap aggregation)  
2. Random forests (many trees = a forest)  
3. Boosting

## Bagging

Bagging is a 4 step process:  

1. Generate B bootstrap samples from the training set.  

2. Construct decision trees for all B bootstrap samples.  

3. For each given test observation, we record the class predicted by each of the B trees.  

4. The overall prediction is the most commonly occuring class among the B predictions. Majority voting wins.

Bagging averages many trees so it reduces the variance of the instability of generating just one tree. Bagging leads to improved prediction. The tradeoff is you lose interpretability and the ability to see simple structure in a tree.


```{r}
library(randomForest)
set.seed(123) 

#Set mtry to equal all predictors. This means all predictors should be considered at each split. This is what makes it "bagging." The default number of trees is 500. 

BC.bag <- randomForest(Class~., mtry=9, data=BC_train_caret, na.action=na.omit, importance=TRUE)
```

### Out of Bag (OOB) Error

A note on the out-of-bag (OOB) error is warranted. OOB is a measure of the test error popular in tree algorithms that use bootstrapping. Gareth et al. (2013) explained OOB as follows:

`Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One that can show that **on average, each bagged tree makes use of around two-thirds of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as out-of-bag (OOB) observations.** We can predict the response for the ith observation using each of the trees in which that observation was OOB. This will yield around B/3 predictions for the ith observation. In order to obtain a single prediction for the ith observation, we take majority vote. This lead to a single OOB prediction for the ith observation. An OOB prediction can be obtained in this way for each of the n observations, from which the overall OOB classification error can be computed. The resulting OOB error is a valid estimate of the test error for the bagged model, since the response for each observation is predicted using only the trees that were not fit using that observation....**It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error**"(p. 317-318).`

```{r}
print(BC.bag) #note the "out of bag" (OOB) error rate. 
```

### What are the important predictors in our bagging model? 

Look at the mean decrease in accuracy of predictions in the OOB samples when a given variable is excluded.

```{r}
importance(BC.bag, type=1)
```

Look at the mean decrease in node impurity resulting from splits over that variable.  

```{r}
importance(BC.bag, type=2)
varImpPlot(BC.bag)
```

```{r}
actual <- BC_validate_caret$Class 
BC_predicted <- predict(BC.bag, newdata=BC_validate_caret, type="class") 
BC_results.matrix.bag <- confusionMatrix(BC_predicted, actual, positive="malignant") 
print(BC_results.matrix.bag)
```


## Random Forest

Random forests consider only a subset of the predictors at each split. This means the node splits are not dominated by one or a few strong predictors, and, thus, give other (i.e. less strong) predictors more chances to be used. When we average the resulting trees, we get more reliable results since the individual trees are not dominated by a few strong predictors.

```{r}
BC.RForest <- randomForest(Class ~.,data=BC_train_caret, mtry=3, ntree=500,na.action = na.omit, importance=TRUE) #default to try three predictors at a time and create 500 trees. 
print(BC.RForest) 
importance(BC.RForest) 
varImpPlot(BC.RForest) 

actual <- BC_validate_caret$Class 
BC_predicted <- predict(BC.RForest, newdata=BC_validate_caret, type="class") 
BC_results.matrix.rf <- confusionMatrix(BC_predicted, actual, positive="malignant") 
print(BC_results.matrix.rf)
```
Our model is "too perfect." 


## Boosting

The boosting model involves:

* We fit a decision tree to the entire training set.   

* We "boost" the observations that were misclassified by giving them higher weights. We fit another decision tree for these misclassified cases.   

* We add the new tree to the existing tree to update the misclassified cases. 

Note that the trees are that built later depend greatly on the trees already built. Learning slowly has shown to improve model accuracy while holding down variability.

```{r}
library(adabag) #a popular boosting algorithm
set.seed(123)
BC_adaboost <- boosting.cv(Class ~.,data=BC_train_caret, boos=TRUE, v=10) #.cv is adding cross validation
#don't worry about warning message. Also, this take a while to run.
BC_adaboost$confusion #confusion matrix for boosting
BC_adaboost$error #error rate for boosting (OOB)
1-BC_adaboost$error #accuracy rate for boosting (OOB)
```

# ROC Curve: One More Performance Evaluation Metric 

The ROC (receiver operating characteristics) curve displays the true positive rate (sensitivity) against the false positive rate (1-specificity). The closer the curve follows the left hand border and then the top left border of the ROC space, the more accurate the model.

```{r}
#Create a ROC curve
library(ROCR)
BC.RForest_predict_prob<-predict(BC.RForest, type="prob", BC_validate_caret)# same as above for predict, but add "prob".
BC.pred = prediction(BC.RForest_predict_prob[,2],BC_validate_caret$Class)#use [,2] to pick the malignant class prob
BC.RForest.perf = performance(BC.pred,"tpr","fpr") #true pos and false pos
plot(BC.RForest.perf ,main="ROC Curve for Random Forest",col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")

unlist(BC.RForest.perf@y.values) #This is the AUC value (area under the ROC curve)
```

# Automatic Selection Features Using doSNOW and RandomForest

```{r}
#rerun random forest with some additional parameters
#Using repeated cross validation
library(caret)
library(doSNOW) #to parallel the process
registerDoSNOW(makeCluster(4, type = "SOCK")) #sets to use all 4 cores in my laptop
cvCtrl <- trainControl(method = "repeatedcv",
                       number = 10, repeats = 5) #sets crossvalidation

set.seed(123)
BC_rf_trained <- train(Class ~.,data=BC_train_caret, method = "rf",
                       metric = "Accuracy", trControl = cvCtrl) #method is randomforest. 

print(BC_rf_trained)
```

# Another Approach: Auto-Tuning to Find Mtry

We can also use a grid search (call "auto tune") to find number of mtry. 

```{r}
grid_rf <- expand.grid(.mtry = c(2,3,4,5,6,7,8,9)) #mtry is how many variables it randomly tries at each node

set.seed(123)
ptm <- proc.time() #starts to time
BC_rf_trained2 <- train(Class ~.,data=BC_train_caret, method = "rf", ntree=500,
                        metric = "Kappa", trControl = cvCtrl, tuneGrid = grid_rf) #also control mtry
print(BC_rf_trained2)
proc.time() - ptm
```

      

=======
---
title: "Week 6"
author: "Xuan Pham"
date: "`r Sys.Date()`"
output: html_document
---

# R Packages 

The packages you will need to install for this week include **caret**, **RandomForest**, **mlbench**, **adabag**, **ROCR**, and **doSNOW**.

# Model Specification & Selection: Questions to Ask

  1. What's my intended goal/outcome?    
  2. What data do I have?    
  3. Which predictors are significant?  
**4. What's the expected performance of a model?**  
**5. How do I improve a model's performance?**

We will focus on Questions 4 & 5 tonight. 

# What's the Expected Performance of a Model? 

The best way to measure performance is to know the **true error rate**. The true error rate is calculated by comparing the model's predictions against actual outcomes in the **entire population**.  In reality, we usually are not working with the whole population. We are working with one or more samples from the population. 

## Naive Approach 

A *naive* way to estimate the true error rate is to apply our model to the entire sample (i.e. training dataset) and then calculate the error rate. The naive approach has several drawbacks:

* Final model will overfit the training data. The problem is magnified when a model has a large number of parameters.    
* Estimated error rate is likely to be lower than the true error rate.

A better approach than the naive method is resampling.   

# Resampling   

Resampling refers to drawing repeated samples from the sample(s) we have. The goal of resampling is to gauge performances of competing models. Resampling is our attempt to simulate the conditions needed to calculate the true error rate.  

We will discuss three resampling methods this week:  

1. one-fold cross validation  
2. k-fold cross validation    
3. leave-one-out cross validation    
4. bootstrapping  


## Validation Set

We discussed validation set approach in Week 2 when we covered decision trees. In particular, the validation set approach involves randomly dividing the available observations into two subgroups: a) a **training set** and b) a **validation set**. We fit our model with the training set and then tests the model's performance on the validation set. Common splits include 60-40 (60% training set and 40% test set), 70-30, and 80-20.

### Test Set

The validation set is there to help us simulate the process of estimating the true error true. The validation set effectively becomes part of the training set when we use it to gauge our model's performance. If we want to measure the expected performance of a model, we should also deploy a third subset called a "test set." 


# Opioid Prescriber: A Redux!

Remember that our target variable is Opioid.Prescriber, where "1" means a practitioner is a frequent opioid prescriber and "0" means s/he is not a frequent opioid prescriber.  

```{r}
setwd("C:/Users/PhamX/Courses/Spring_2017/BIA_6301/Module_6/data")
prescribers<-read.csv("prescribers.csv")

#View(prescribers)

prescribers<-prescribers[,c(241,1:240,242:331)] #Rearranging the columns so that our target variable is first

dim(prescribers)
#names(prescribers)

table(prescribers$Opioid.Prescriber)
```

Let's do a training set of 80% and validation set of 20%. We will build a decision tree model on 80% of the data set and then test the model's performance on the other 20% of the data set.   

```{r}
set.seed(123) #set a seed to do draws from a random uniform distribution.
prescribers_rand <- prescribers[order(runif(25000)), ] 
prescribers_train <- prescribers_rand[1:20000, ] #Training data set; 20000 observations
prescribers_validate  <-prescribers_rand[20001:25000, ]
```

Checking the proportions of opioid prescriber in the trainining and validation sets. They are roughly the same. 58.8% in training set are opioid prescribers; 58.3% in validation set are opioid prescribers.

```{r}
dim(prescribers_train) #checking the split
dim(prescribers_validate) #checking the split
prop.table(table(prescribers_train$Opioid.Prescriber)) #checking to see the class proportions between the training and test sets. 
prop.table(table(prescribers_validate$Opioid.Prescriber))
```


```{r}
library(rpart)
library(rpart.plot)

set.seed(123)
prescribers_rpart <- rpart(prescribers_train$Opioid.Prescriber~., method="class", parms = list(split="gini"), data=prescribers_train)

plot(prescribers_rpart, uniform=TRUE, main="Classification Tree for Opioid Prescribers")
text(prescribers_rpart, use.n=TRUE, all=TRUE, cex=0.8)


# Something a bit fancier
library(rpart.plot)
rpart.plot(prescribers_rpart, type=0, extra=101)
rpart.plot(prescribers_rpart, type=1, extra=101)
```

```{r}
library(caret)
actual <- prescribers_validate$Opioid.Prescriber
predicted <- predict(prescribers_rpart, prescribers_validate, type="class")
results.matrix <- confusionMatrix(predicted, actual, positive="yes")
print(results.matrix)
```
Accuracy rate is 76%, but the sensitivity (how well does the decision tree classified frequent opioid prescribers correctly) is 70%. Furthermore, the specificity (how well does the decision tree classified non-frequent opioid prescribers correctly) is 84%. Our decision tree model does a better job classifying the non-frequent opioid prescribers than the frequent opioid prescribers. 


### Using the Caret Package Train & Validate Models

```{r}
set.seed(123)
trainIndex <- createDataPartition(prescribers$Opioid.Prescriber, p = .8,list = FALSE,times = 1)
prescribers_train_caret <- prescribers[ trainIndex,]
prescribers_validate_caret <- prescribers[ -trainIndex,]

```

```{r}

prescribers_rpart_caret <- rpart(Opioid.Prescriber~., method="class", parms = list(split="gini"), data=prescribers_train_caret)

plot(prescribers_rpart_caret, uniform=TRUE, main="Classification Tree for Opioid Prescribers")
text(prescribers_rpart_caret, use.n=TRUE, all=TRUE, cex=0.8)

rpart.plot(prescribers_rpart_caret, type=0, extra=101)
rpart.plot(prescribers_rpart_caret, type=1, extra=101)

actual <- prescribers_validate_caret$Opioid.Prescriber
predicted <- predict(prescribers_rpart_caret, prescribers_validate_caret, type="class")
results.matrix <- confusionMatrix(predicted, actual, positive="yes")
print(results.matrix)
```

Accuracy = 78%.  
Sensitivity = 79%  
Specificity = 74%  

The caret approach creates a test set that shows the decision tree is doing slightly better at identifying opioid prescribers. 

## Revisiting the Bias-Variance Discussion

Alpayin (2010) provided a great visualization of the bias-variance problem:  

[!]http://images.slideplayer.com/15/4846480/slides/slide_7.jpg 

When we are estimating the true error rate, we need to remember that we are dealing with the bias & variance problem too. In one fold cross validation, we have an estimated error rate that is both biased and has high variance. One way to reduce both the bias and variance is by using k-fold cross validation.


## k-Fold Cross Validation

k-fold cross validation is a resampling technique that divides the dataset into k groups, or folds, of equal size. Here is how it works:  

1. Keep one fold as the validation set. Fit the model on the other k-1 folds.  

2. Test fitted model on the validation set. Calculate the mean squared error (MSE) on the validation set. 

3. Repeat Steps 1 & 2 over and over again so that a different fold is used as a validation set. **The true error rate is estimated as the average error rate of all repetitions.**  

Use the **caret** package for this task.  

We will divide the training set into 10-folds. Each fold will eventually be used as a validation set.

```{r}

fitControl <- trainControl(method="cv", number=10) #10-fold cross validation

set.seed(123)
prescribers_10folds<-train(Opioid.Prescriber~., data=prescribers_train_caret, method="rpart", metric="Accuracy", trControl=fitControl)
prescribers_10folds
```

Using the cp value to prune our tree (if needed)
```{r}
#prescribers_rpart_pruned_10folds<-prune(prescribers_10folds, cp=0.06)
#rpart.plot(prescribers_rpart_pruned_10folds)
```

Now we calculate the error rate of the chosen decision tree on the validation set. 

```{r}
actual <- prescribers_validate_caret$Opioid.Prescriber
predicted <- predict(prescribers_10folds, prescribers_validate_caret, type="raw")
results.matrix <- confusionMatrix(predicted, actual, positive="yes")
print(results.matrix)
```

Sensitivity = 61%; Specificity = 89%


### Kappa? 

$$ Kappa = \frac{Pr(a) - Pr(e)}{1-Pr(e)} $$

Where, 

Pr(a): proportion of actual agreement between the classifier and the true values  

Pr(e): proportion of expected agreement between the classifier and the true values

Kappa "adjusts accuracy by accounting for the possibility of a correct prediction by **chance alone.** Kappa values range to a maximum number of 1, which indicates perfect agreement between the model's predictions and the true values--a rare occurrence. Values less than one indicate imperfect agreement" (Lantz 2013, p. 303)


|                   |Actual  |        |Marginal_Frequency    |
|------------------:|-------:|-------:|---------------------:|
|Prediction         |NO      |  YES   |                      |
|NO                 |1829    |1157    |2986                  |
|YES                |233     |1780    |2013                  |
|-------------------|--------|--------|----------------------|
|Marginal_Frequency |2062    |2937    |                      |


```{r}
Observed_Accuracy = (1829+1780)/5000 
Expected_Accuracy_NO = (2986*2062)/5000
Expected_Accuracy_YES = (2031*2937)/5000
Expected_Accuracy_BOTH_CLASSES = (Expected_Accuracy_NO+Expected_Accuracy_YES)/5000
Kappa_Statistic = (Observed_Accuracy-Expected_Accuracy_BOTH_CLASSES)/(1-Expected_Accuracy_BOTH_CLASSES)

table<-cbind(Observed_Accuracy,Expected_Accuracy_NO,Expected_Accuracy_YES,Expected_Accuracy_BOTH_CLASSES, Kappa_Statistic)

table_t<-t(table)

colnames(table_t)<-c("value")


library(knitr)
kable(table_t)
```

#### Side Note on Kappa Statistic

What's a good kappa value?

Landis and Koch (1977): 0-20 => slight, 0.21-.40 => fair, 0.41-0.6 => moderate, 0.61-0.8 => substantial, 0.81 -1 => almost perfect  

Fleiss (1981): 0-.40 => poor; 0.41-0.75 => fair to good; 0.75 – 1 => excellent.

Be careful! Kappa is not the best metric if accuracy is not what you are after. 


### Repeated k-fold Cross Validation

k-fold cross validation is still problematic, however. Vanwinckelen and Blockeel (2011) noted:  

`In addition to bias, the results of a k-fold cross-validation also have high variance. If we run two different tenfold cross-validations for the same learners on the same daa set S, but with different random partitioning of S into subsets S(i), these two cross-validations can give quite different results. An estimate with smaller variance can be obtained by repeating the cross-validation several times, with different partitionings, and taking the average of the results obtained during each cross-validation (page 2).`


```{r}
fitControl <- trainControl(method="cv", number=10, repeats=5) #10-fold cross validation #repeated 5 times.

set.seed(123)
prescribers_10folds_rp<-train(Opioid.Prescriber~., data=prescribers_train_caret, method="rpart", metric="Accuracy", trControl=fitControl)
prescribers_10folds_rp

actual <- prescribers_validate_caret$Opioid.Prescriber
predicted <- predict(prescribers_10folds_rp, prescribers_validate_caret, type="raw")
results.matrix <- confusionMatrix(predicted, actual, positive="yes")
print(results.matrix)
```


## Leave-one-out Cross Validation (LOOCV)

Repeated k-fold cross validation can help reduce the high variance problem, but we still have to deal with the high bias problem. A way to minimize the bias problem is to do LOOCV. The technique is a degenerate case of k-fold cross validation, where K is chosen as the total number of observations. LOOCV uses all observations as the training set and leaves one observation out as the test set. The process repeats until all observations have been used as a validation set.

LOOCV is very computationally intensive!!

```{r}

#fitControl <- trainControl(method="LOOCV") #10-fold cross validation

#set.seed(123)
#prescribers_loocv<-train(Opioid.Prescriber~., data=prescribers_train_caret, method="rpart", metric="Accuracy", trControl=fitControl)
#prescribers_loocv
```

```{r}
#actual <- prescribers_validate_caret$Opioid.Prescriber
#predicted <- predict(prescribers_loocv, prescribers_validate_caret, type="raw")
#results.matrix <- confusionMatrix(predicted, actual, positive="yes")
#print(results.matrix)
```

## Bootstrapping 

Bootstrapping is a resampling technique that obtain distinct datasets by repeatedly sampling observations from the original dataset with replacement. 

Each boostrapped dataset is created by sampling with replacement and is the same size as the original dataset. Consequently, some observations may appear more than once in a given boostrapped dataset while other observations may not appear at all.

Note: The default method in the train() function in the caret package is the bootstrap.

```{r}
cvCtrl <- trainControl(method="boot", number=10) #10 bootstrapped samples.
set.seed(123)
prescribers_bootstrap<-train(Opioid.Prescriber~., data=prescribers_train_caret, method="rpart", metric="Accuracy", trControl=cvCtrl)
prescribers_bootstrap
```

```{r}
actual <- prescribers_validate_caret$Opioid.Prescriber
predicted <- predict(prescribers_bootstrap, prescribers_validate_caret, type="raw")
results.matrix <- confusionMatrix(predicted, actual, positive="yes")
print(results.matrix)
```


## Last Words on Resampling

A question you may be pondering about is "how many folds should I use?" The answer depends on the size of the dataset. For large datasets, you can use a small number of folds and still get an accurate test error estimate. For smaller datasets, you may have to use LOOCV. You should remember these rules:


**Small number of folds** = error estimate is more biased but also has lower variance. Computationally less intensive.   

**Large number of folds** = error estimate is less biased but also has higher variance. More computationally intensive. 


# How Do I Improve Model Performance? 

For the second part of this class, we will use the **Breast Cancer** data set from the mlbench package. It's a smaller data set so easier for us to handle in class.

```{r}
library(mlbench)
data("BreastCancer")
str(BreastCancer)
BC<-BreastCancer[-1] #removing ID column
```

```{r}
set.seed(123)
trainIndex <- createDataPartition(BC$Class, p = .8,list = FALSE,times = 1)
head(trainIndex)
BC_train_caret <- BC[trainIndex,]
BC_validate_caret <- BC[trainIndex,]
```


```{r}
BC_rpart_caret <- rpart(BC_train_caret$Class~., method="class", parms = list(split="gini"), data=BC_train_caret)

plot(BC_rpart_caret, uniform=TRUE, main="Classification Tree for Opioid Prescribers")
text(BC_rpart_caret, use.n=TRUE, all=TRUE, cex=0.8)

rpart.plot(BC_rpart_caret, type=0, extra=101)
rpart.plot(BC_rpart_caret, type=1, extra=101)

actual <- BC_validate_caret$Class
predicted <- predict(BC_rpart_caret, BC_validate_caret, type="class")
results.matrix <- confusionMatrix(predicted, actual, positive="malignant")
print(results.matrix)
```

Our decision tree model does a slightly better job at predicting malignant tumors (true positives) (97%) than benign tumors (96%). This is what we want to see!


# Improving Model Performance: Ensemble Models Approach

One decision tree suffers from high variance. The resulting tree depends on the training data. What we want is a procedure with low variance--meaning we should see similar results if the tree is applied repeatedly to distinct datasets. We will examine three ensemble models that are built on the basic decision trees:

1. Bagging (bootstrap aggregation)  
2. Random forests (many trees = a forest)  
3. Boosting

## Bagging

Bagging is a 4 step process:  

1. Generate B bootstrap samples from the training set.  

2. Construct decision trees for all B bootstrap samples.  

3. For each given test observation, we record the class predicted by each of the B trees.  

4. The overall prediction is the most commonly occuring class among the B predictions. Majority voting wins.

Bagging averages many trees so it reduces the variance of the instability of generating just one tree. Bagging leads to improved prediction. The tradeoff is you lose interpretability and the ability to see simple structure in a tree.


```{r}
library(randomForest)
set.seed(123) 

#Set mtry to equal all predictors. This means all predictors should be considered at each split. This is what makes it "bagging." The default number of trees is 500. 

BC.bag <- randomForest(Class~., mtry=9, data=BC_train_caret, na.action=na.omit, importance=TRUE)
```

### Out of Bag (OOB) Error

A note on the out-of-bag (OOB) error is warranted. OOB is a measure of the test error popular in tree algorithms that use bootstrapping. Gareth et al. (2013) explained OOB as follows:

`Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One that can show that **on average, each bagged tree makes use of around two-thirds of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as out-of-bag (OOB) observations.** We can predict the response for the ith observation using each of the trees in which that observation was OOB. This will yield around B/3 predictions for the ith observation. In order to obtain a single prediction for the ith observation, we take majority vote. This lead to a single OOB prediction for the ith observation. An OOB prediction can be obtained in this way for each of the n observations, from which the overall OOB classification error can be computed. The resulting OOB error is a valid estimate of the test error for the bagged model, since the response for each observation is predicted using only the trees that were not fit using that observation....**It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error**"(p. 317-318).`

```{r}
print(BC.bag) #note the "out of bag" (OOB) error rate. 
```

### What are the important predictors in our bagging model? 

Look at the mean decrease in accuracy of predictions in the OOB samples when a given variable is excluded.

```{r}
importance(BC.bag, type=1)
```

Look at the mean decrease in node impurity resulting from splits over that variable.  

```{r}
importance(BC.bag, type=2)
varImpPlot(BC.bag)
```

```{r}
actual <- BC_validate_caret$Class 
BC_predicted <- predict(BC.bag, newdata=BC_validate_caret, type="class") 
BC_results.matrix.bag <- confusionMatrix(BC_predicted, actual, positive="malignant") 
print(BC_results.matrix.bag)
```


## Random Forest

Random forests consider only a subset of the predictors at each split. This means the node splits are not dominated by one or a few strong predictors, and, thus, give other (i.e. less strong) predictors more chances to be used. When we average the resulting trees, we get more reliable results since the individual trees are not dominated by a few strong predictors.

```{r}
BC.RForest <- randomForest(Class ~.,data=BC_train_caret, mtry=3, ntree=500,na.action = na.omit, importance=TRUE) #default to try three predictors at a time and create 500 trees. 
print(BC.RForest) 
importance(BC.RForest) 
varImpPlot(BC.RForest) 

actual <- BC_validate_caret$Class 
BC_predicted <- predict(BC.RForest, newdata=BC_validate_caret, type="class") 
BC_results.matrix.rf <- confusionMatrix(BC_predicted, actual, positive="malignant") 
print(BC_results.matrix.rf)
```
Our model is "too perfect." 


## Boosting

The boosting model involves:

* We fit a decision tree to the entire training set.   

* We "boost" the observations that were misclassified by giving them higher weights. We fit another decision tree for these misclassified cases.   

* We add the new tree to the existing tree to update the misclassified cases. 

Note that the trees are that built later depend greatly on the trees already built. Learning slowly has shown to improve model accuracy while holding down variability.

```{r}
library(adabag) #a popular boosting algorithm
set.seed(123)
BC_adaboost <- boosting.cv(Class ~.,data=BC_train_caret, boos=TRUE, v=10) #.cv is adding cross validation
#don't worry about warning message. Also, this take a while to run.
BC_adaboost$confusion #confusion matrix for boosting
BC_adaboost$error #error rate for boosting (OOB)
1-BC_adaboost$error #accuracy rate for boosting (OOB)
```

# ROC Curve: One More Performance Evaluation Metric 

The ROC (receiver operating characteristics) curve displays the true positive rate (sensitivity) against the false positive rate (1-specificity). The closer the curve follows the left hand border and then the top left border of the ROC space, the more accurate the model.

```{r}
#Create a ROC curve
library(ROCR)
BC.RForest_predict_prob<-predict(BC.RForest, type="prob", BC_validate_caret)# same as above for predict, but add "prob".
BC.pred = prediction(BC.RForest_predict_prob[,2],BC_validate_caret$Class)#use [,2] to pick the malignant class prob
BC.RForest.perf = performance(BC.pred,"tpr","fpr") #true pos and false pos
plot(BC.RForest.perf ,main="ROC Curve for Random Forest",col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")

unlist(BC.RForest.perf@y.values) #This is the AUC value (area under the ROC curve)
```

# Automatic Selection Features Using doSNOW and RandomForest

```{r}
#rerun random forest with some additional parameters
#Using repeated cross validation
library(caret)
library(doSNOW) #to parallel the process
registerDoSNOW(makeCluster(4, type = "SOCK")) #sets to use all 4 cores in my laptop
cvCtrl <- trainControl(method = "repeatedcv",
                       number = 10, repeats = 5) #sets crossvalidation

set.seed(123)
BC_rf_trained <- train(Class ~.,data=BC_train_caret, method = "rf",
                       metric = "Accuracy", trControl = cvCtrl) #method is randomforest. 

print(BC_rf_trained)
```

# Another Approach: Auto-Tuning to Find Mtry

We can also use a grid search (call "auto tune") to find number of mtry. 

```{r}
grid_rf <- expand.grid(.mtry = c(2,3,4,5,6,7,8,9)) #mtry is how many variables it randomly tries at each node

set.seed(123)
ptm <- proc.time() #starts to time
BC_rf_trained2 <- train(Class ~.,data=BC_train_caret, method = "rf", ntree=500,
                        metric = "Kappa", trControl = cvCtrl, tuneGrid = grid_rf) #also control mtry
print(BC_rf_trained2)
proc.time() - ptm
```

      

>>>>>>> f276e6fa3dd75803d5d7f5695cdb203ca32b05ab
